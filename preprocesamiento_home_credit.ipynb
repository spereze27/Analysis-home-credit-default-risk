{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3372d4",
   "metadata": {},
   "source": [
    "# Configuración inicial y librerías\n",
    "\n",
    "Se importan las librerías necesarias para llevar a cabo el preprocesamiento del dataset. Este conjunto de datos requiere transformación de fechas, codificación de variables categóricas, creación de nuevas características y manejo de valores faltantes. A continuación, se explican brevemente las librerías utilizadas:\n",
    "\n",
    "- `pandas`: manipulación de datos estructurados.\n",
    "- `numpy`: operaciones numéricas y matemáticas.\n",
    "- `scipy.stats`: operaciones estadísticas como la moda.\n",
    "- `sklearn.preprocessing.LabelEncoder`: codificación de variables categóricas.\n",
    "- `matplotlib.pyplot`: visualización básica (opcional).\n",
    "- `warnings`: suprime advertencias innecesarias en la ejecución.\n",
    "- `gc` (garbage collector): optimiza el uso de memoria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4876b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Se configura pandas para mostrar todas las columnas\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Ignorar advertencias\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Activar garbage collector para habilitar la recoleccion de basura, esto mejora el rendimiento en ambientes \n",
    "# que tienen un uso intensivo de memoria como lo es el caso del manejo de los datasets de la actividad desarrollada\n",
    "\n",
    "gc.enable()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada1f87",
   "metadata": {},
   "source": [
    "# Funciones para el preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748dd303",
   "metadata": {},
   "source": [
    "### Función para contar valores nulos\n",
    "La función `contar_nulos(data)` permite encontrar valores faltantes en un conjunto de datos. Su objetivo es identificar qué columnas tienen datos incompletos, cuántos valores faltan y qué porcentaje representan respecto al total de registros.\n",
    "\n",
    "Primero, calcula la cantidad total de valores nulos en cada columna utilizando `data.isnull().sum()`, que cuenta los `True` para cada celda nula. Luego, ordena estas cantidades de forma descendente para priorizar las columnas más problemáticas.\n",
    "\n",
    "A continuación, calcula el porcentaje de nulos dividiendo cada total de nulos por el número total de filas del `DataFrame`, y multiplica por 100 para expresarlo en porcentaje. También se ordenan los resultados de mayor a menor.\n",
    "\n",
    "Ambas series (total y porcentaje) se combinan en un solo `DataFrame` usando `pd.concat.\n",
    "\n",
    "Por último, se filtran las columnas con valores nulos, devolviendo únicamente aquellas que presentan al menos un valor faltante. Esto permite enfocarse exclusivamente en las variables que requieren algún tipo de tratamiento durante la limpieza de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1afb1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contar_nulos(data):\n",
    "    total = data.isnull().sum().sort_values(ascending=False)\n",
    "    porcentaje = (data.isnull().sum() / len(data) * 100).sort_values(ascending=False)\n",
    "    tabla = pd.concat([total, porcentaje], axis=1, keys=[\"Total de nulos\", \"Porcentaje\"])\n",
    "    return tabla[tabla[\"Total de nulos\"] > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49015e99",
   "metadata": {},
   "source": [
    "### Función convertir_dias\n",
    "La función `convertir_dias(data, columnas, divisor=12, redondear=True, reemplazar=False)` se utiliza para transformar columnas que contienen valores de tiempo expresados en días negativos, como por ejemplo DAYS_BIRTH o DAYS_EMPLOYED, en unidades más interpretables como meses o años.\n",
    "\n",
    "El parámetro columnas especifica la lista de variables que se desean transformar. El parámetro `divisor` determina en qué unidad se hará la conversión (en este caso 12, para convertirlo a meses).\n",
    "\n",
    "Dentro del bucle, se invierte el signo del valor (porque los días vienen negativos en el dataset), se divide por el `divisor`. Luego, se eliminan los valores negativos resultantes, ya que estos suelen ser errores de codificación y se reemplazan por None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b6fc1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convertir_dias(data, columnas, divisor=12, redondear=True, reemplazar=False):\n",
    "    for col in columnas:\n",
    "        nueva_columna = \"CONVERTIDO_\" + str(col)\n",
    "        valores_convertidos = -data[col] / divisor\n",
    "        if redondear:\n",
    "            valores_convertidos = valores_convertidos.round()\n",
    "        valores_convertidos[valores_convertidos < 0] = None\n",
    "        if reemplazar:\n",
    "            data[col] = valores_convertidos\n",
    "        else:\n",
    "            data[nueva_columna] = valores_convertidos\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de15ad8",
   "metadata": {},
   "source": [
    "### Función para crear variables logarítmicas\n",
    "La función `crear_logaritmos(data, columnas, reemplazar=False)` se utiliza para aplicar una transformación logarítmica a una o más variables numéricas del dataset. Este tipo de transformación es común cuando se trabaja con variables con distribuciones altamente sesgadas (como ingresos, montos de crédito, etc.), esto se debe a que las transformaciones logarítmicas pueden ayudar a corregir la asimetría de variables y mejorar la linealidad en modelos estadísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e21f7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def crear_logaritmos(data, columnas, reemplazar=False):\n",
    "    for col in columnas:\n",
    "        valores_log = np.log(data[col].abs() + 1)\n",
    "        if reemplazar:\n",
    "            data[col] = valores_log\n",
    "        else:\n",
    "            data[\"LOG_\" + str(col)] = valores_log\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ef6dd",
   "metadata": {},
   "source": [
    "### Función para crear indicadores flags de nulos\n",
    "``crear_flags_nulos(data, columnas=None)`` tiene como propósito generar nuevas variables binarias (0 o 1) que indiquen si un valor estaba originalmente ausente (nulo) en el dataset. Esto con el fin de que la ausencia de valores se vuelva una informacion relevante para el modelo.\n",
    "Por ejemplo, si los clientes con ingresos faltantes tienden a incumplir, el simple hecho de no declarar el ingreso se vuelve información sumamente relevante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4415d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crear_flags_nulos(data, columnas=None):\n",
    "    if columnas is None:\n",
    "        columnas = data.columns\n",
    "    for col in columnas:\n",
    "        if data[col].isnull().any():\n",
    "            data[\"ES_NULO_\" + str(col)] = data[col].isnull().astype(int)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c9634",
   "metadata": {},
   "source": [
    "### Función para codificar variables categóricas\n",
    "Mucha de la información que se tiene del dataset son string o cadenas de texto que el modelo no va a poder interpretar por si solo, se crea la funcion `tratar_categoricas(data)` para que a cada columna que tenga valores de tipo objeto se le reemplace cada categoria por un valor entero unico.\n",
    "Por ejemplo en la columna `GENRE` se remplazaria ``M`` por 0 y ``F`` por 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dbba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tratar_categoricas(data):\n",
    "\n",
    "    #Codifica todas las variables categóricas del DataFrame usando codificación por etiquetas.\n",
    "    #Cada categoría única se reemplaza por un número entero único.\n",
    "\n",
    "    columnas_obj = [col for col in data.columns if data[col].dtype == \"object\"]\n",
    "    for col in columnas_obj:\n",
    "        data[col], _ = pd.factorize(data[col])\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02df23",
   "metadata": {},
   "source": [
    "### Función para calcular tasas de aprobación y rechazo\n",
    "Despues de realizar una busqueda sobre modelos de riesgo crediticio se encontro que los modelos de crédito deben de considerar el historial de comportamiento previo como uno de los predictores más potentes.\n",
    "\n",
    "Esto se puede encontrar en Basel Committee on Banking Supervision — Credit Risk Modelling: Current Practices and Applications (https://www.bis.org/publ/bcbs49.htm) donde se menciona expresamente el uso de variables como el comportamiento pasado del cliente como feature principal o en Anderson, R. (2007). The Credit Scoring Toolkit: Theory and Practice for Retail Credit Risk Management (mirar capitulo 9) donde se menciona que variables como número de rechazos previos, historial de solicitudes y frecuencia de aceptación son claves para desarrollar un analisis completo.\n",
    "\n",
    "Se procede a construir la funcion `calcular_ratio_aprobacion(data, lags=[1, 2, 5])` que tiene como objetivo calcular, para cada cliente ``(SK_ID_CURR)``, el porcentaje de solicitudes aprobadas y rechazadas en sus últimas t solicitudes de crédito, donde t toma los valores definidos en la lista lags (para este caso elegi los valores: [1, 2, 5]).\n",
    "\n",
    "se crearan las columnas \"RATIO_APROBADO_{t}\" donde RATIO_APROBADO_1 es si la ultima solicitud fue aprobada y RATIO_APROBADO_2 es cuántas de las 2 más recientes fueron rechazadas, en promedio. Exactamente lo mismo para el ratio de rechazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calcular_ratio_aprobacion(data, lags=[1, 2, 5]):\n",
    "    #Se extraen solo las columnas necesarias\n",
    "    df = data[[\"SK_ID_CURR\", \"SK_ID_PREV\", \"DAYS_DECISION\", \"NAME_CONTRACT_STATUS\"]].copy()\n",
    "    #La columna DAYS_DECISION representa la antigüedad de la solicitud (en días negativos). \n",
    "    #Se invierte el signo para que los valores más recientes queden al final, y se ordena por cliente y fecha.\n",
    "    df[\"DAYS_DECISION\"] = -df[\"DAYS_DECISION\"]\n",
    "    df = df.sort_values([\"SK_ID_CURR\", \"DAYS_DECISION\"])\n",
    "    #Se aplican dummies sobre la columna NAME_CONTRACT_STATUS para generar columnas binarias como: 1 aprobado o 0 rechazado\n",
    "    df = pd.get_dummies(df)\n",
    "\n",
    "    for t in lags:\n",
    "        tmp = df.groupby(\"SK_ID_CURR\")[\"NAME_CONTRACT_STATUS_Approved\"].head(t).groupby(\"SK_ID_CURR\").mean().reset_index()\n",
    "        tmp.columns = [\"SK_ID_CURR\", f\"RATIO_APROBADO_{t}\"]\n",
    "        data = data.merge(tmp, on=\"SK_ID_CURR\", how=\"left\")\n",
    "\n",
    "        tmp = df.groupby(\"SK_ID_CURR\")[\"NAME_CONTRACT_STATUS_Refused\"].head(t).groupby(\"SK_ID_CURR\").mean().reset_index()\n",
    "        tmp.columns = [\"SK_ID_CURR\", f\"RATIO_RECHAZADO_{t}\"]\n",
    "        data = data.merge(tmp, on=\"SK_ID_CURR\", how=\"left\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ce2b8",
   "metadata": {},
   "source": [
    "### Función para agregar datos numéricos y categóricos por cliente\n",
    "La funcion `agregar_por_cliente(data, id_col, etiqueta=None)` se crea con el objetivo de separar las columnas en 2 tipos:\n",
    "* Categoricas\n",
    "* Numericas\n",
    "\n",
    "Esto con el objetivo de poder extraer los estadisticos relevantes de estos 2 tipos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067eeba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.stats\n",
    "\n",
    "def agregar_por_cliente(data, id_col, etiqueta=None):\n",
    "    #Se separan las columnas en categóricas y numéricas. Las categóricas son aquellas con tipo de dato object, y el resto son numéricas.\n",
    "    categoricas = [col for col in data.columns if data[col].dtype == \"object\"]\n",
    "    datos_numericos = data.drop(columns=categoricas)\n",
    "    datos_categoricos = data[[id_col] + categoricas]\n",
    "\n",
    "    print(f\"- Variables categóricas: {len(categoricas)}\")\n",
    "    print(f\"- Variables numéricas: {datos_numericos.shape[1] - 1}\")\n",
    "\n",
    "    # Para las columnas numéricas, se agrupan por el id_col y se calculan varias estadísticas: \n",
    "    # la media (mean), la desviación estándar (std), el valor mínimo (min), y el valor máximo (max)\n",
    "    if datos_numericos.shape[1] > 1:\n",
    "        numericos_agg = datos_numericos.groupby(id_col).agg([\"mean\", \"std\", \"min\", \"max\"])\n",
    "        numericos_agg.columns = [f\"{col[0]}_{col[1]}\" for col in numericos_agg.columns]\n",
    "        numericos_agg = numericos_agg.sort_index()\n",
    "    else:\n",
    "        numericos_agg = pd.DataFrame()\n",
    "\n",
    "    # Para las columnas categóricas, también se agrupan por el id_col, y se calculan dos estadísticas:\n",
    "    # la moda y el numero de valores unicos (cuantas categorias tiene una columna)\n",
    "    if categoricas:\n",
    "        categoricos_agg = datos_categoricos.groupby(id_col).agg({\n",
    "            col: [lambda x: scipy.stats.mode(x)[0][0], lambda x: x.nunique()]\n",
    "            for col in categoricas\n",
    "        })\n",
    "        categoricos_agg.columns = [f\"{col[0]}_{col[1]}\" for col in categoricos_agg.columns]\n",
    "        categoricos_agg = categoricos_agg.sort_index()\n",
    "    else:\n",
    "        categoricos_agg = pd.DataFrame()\n",
    "\n",
    "    # Despues de calcular los estadisticos mas relevantes se unen en un solo Dataframe\n",
    "    if not numericos_agg.empty and not categoricos_agg.empty:\n",
    "        resultado = pd.concat([numericos_agg, categoricos_agg], axis=1)\n",
    "    elif not numericos_agg.empty:\n",
    "        resultado = numericos_agg\n",
    "    else:\n",
    "        resultado = categoricos_agg\n",
    "\n",
    "    if etiqueta:\n",
    "        resultado.columns = [etiqueta + \"_\" + col for col in resultado.columns]\n",
    "\n",
    "    print(\"- Dimensiones finales:\", resultado.shape)\n",
    "    return resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a68e95",
   "metadata": {},
   "source": [
    "### Importar la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/raw/application_train.csv\")\n",
    "test  = pd.read_csv(\"../data/raw/application_test.csv\")\n",
    "bureau  = pd.read_csv(\"../data/raw/bureau.csv\")\n",
    "bureau_balance  = pd.read_csv(\"../data/raw/bureau_balance.csv\")\n",
    "previous_application  = pd.read_csv(\"../data/raw/previous_application.csv\")\n",
    "credit_card  = pd.read_csv(\"../data/raw/credit_card_balance.csv\")\n",
    "pos_cash  = pd.read_csv(\"../data/raw/POS_CASH_balance.csv\")\n",
    "installments  = pd.read_csv(\"../data/raw/installments_payments.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
